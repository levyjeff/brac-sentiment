{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2997d190",
   "metadata": {},
   "source": [
    "# Exploring text analysis for BRAC\n",
    "\n",
    "Exploring how text analysis might be used on the news articles for studying the uncertainty effect of base closures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc3c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6759d7",
   "metadata": {},
   "source": [
    "## Sample cities with BRAC DoD proposal amounts\n",
    "\n",
    "Data copied from 2005_BRAC_proposals.csv, collected from the 2005 BRAC commission report, Appendix O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcdd5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>jobs_lost</th>\n",
       "      <th>jobs_gained</th>\n",
       "      <th>net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>-6820</td>\n",
       "      <td>124</td>\n",
       "      <td>-6764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baltimore</td>\n",
       "      <td>-4414</td>\n",
       "      <td>11619</td>\n",
       "      <td>7727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lubbock</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norfolk</td>\n",
       "      <td>-10399</td>\n",
       "      <td>10400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norwich</td>\n",
       "      <td>-8460</td>\n",
       "      <td>0</td>\n",
       "      <td>-8460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>-812</td>\n",
       "      <td>7821</td>\n",
       "      <td>7009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>San Antonio</td>\n",
       "      <td>-8129</td>\n",
       "      <td>10946</td>\n",
       "      <td>2817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>-283</td>\n",
       "      <td>3592</td>\n",
       "      <td>3309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city  jobs_lost  jobs_gained   net\n",
       "0       Atlanta      -6820          124 -6764\n",
       "1     Baltimore      -4414        11619  7727\n",
       "2       Lubbock         -7            0    -7\n",
       "3       Norfolk     -10399        10400     1\n",
       "4       Norwich      -8460            0 -8460\n",
       "5      Richmond       -812         7821  7009\n",
       "6   San Antonio      -8129        10946  2817\n",
       "7  Indianapolis       -283         3592  3309"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['Atlanta', -6820, 124, -6764],\n",
    "        ['Baltimore', -4414, 11619, 7727],\n",
    "        ['Lubbock', -7, 0, -7],\n",
    "        ['Norfolk', -10399, 10400, 1],\n",
    "        ['Norwich', -8460, 0, -8460],\n",
    "        ['Richmond', -812, 7821, 7009],\n",
    "        ['San Antonio', -8129, 10946, 2817],\n",
    "        ['Indianapolis', -283, 3592, 3309]]\n",
    "df_brac = pd.DataFrame(data, columns=['city', 'jobs_lost', 'jobs_gained', 'net'])\n",
    "df_brac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b75c9",
   "metadata": {},
   "source": [
    "# Loading the sample news articles from AWN\n",
    "\n",
    "There are ten articles from places representing a variety of BRAC proposed outcomes, with Baltimore and Norwich reprsented twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d253af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atlanta': 'Local officials are preparing to lobby members of the federal Base Realignment and Closure Commissio',\n",
       " 'Baltimore 1': 'While officials in Maryland have publicly welcomed the prospect of gaining 6,600 federal jobs throug',\n",
       " 'Baltimore 2': 'Like many federal workers driving from the Baltimore suburbs into Washington every day, Marshall Hud',\n",
       " 'Indianapolis': 'Gov. Mitch Daniels expressed relief this morning that the Pentagon plans to keep open the Naval Surf',\n",
       " 'Lubbock': 'HAWTHORNE, Nev. (AP) - For more than 50 years, this struggling desert town that proudly calls itself',\n",
       " 'Norfolk': 'FORT MONROE - There was good news in the bad news.\\n\\nAfter centuries of standing guard, Fort Monroe m',\n",
       " 'Norwich 1': 'NORWICH - The task is simple: Do the math and find the flaw.\\n\\nWhen the Groton submarine base was tar',\n",
       " 'Norwich 2': 'WASHINGTON - While the initial focus of the state has been on the Groton Submarine Base, it is not t',\n",
       " 'Richmond': \"The Prince George Comfort Inn, a half-mile from Fort Lee's main gate, relies on the military for 50 \",\n",
       " 'San Antonio': 'The loss of a military organization at Lackland AFB known as the Cryptologic Support Group is greate'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_article(fname):\n",
    "    #Note: We don't actually know the structure of the text files we would get from purchased\n",
    "    #AWN access, so this function both handles the trivial example data, and serves as a\n",
    "    #placeholder for whatever the future structure looks like.\n",
    "    with open(os.path.join('articles', fname), 'r') as ifile:\n",
    "        text = ifile.read()\n",
    "\n",
    "    #Based on the body of every article beginning after the \"Words\" count, return only the body\n",
    "    text = text[text.find('Words')+len('Words')+2:]\n",
    "    return text\n",
    "\n",
    "articles = {fname[:-4]:load_article(fname) for fname in os.listdir(r'articles')}\n",
    "{city:text[:100] for city, text in articles.items()} #display first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ad14c",
   "metadata": {},
   "source": [
    "# Basic word presence\n",
    "\n",
    "This mirrors the behavior we can get through the AWN interface directly, without any text analysis. This does not require purchasing access to the text of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539f26c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is BRAC discussed? (should be all True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Atlanta': True,\n",
       " 'Baltimore 1': True,\n",
       " 'Baltimore 2': True,\n",
       " 'Indianapolis': True,\n",
       " 'Lubbock': True,\n",
       " 'Norfolk': True,\n",
       " 'Norwich 1': True,\n",
       " 'Norwich 2': True,\n",
       " 'Richmond': True,\n",
       " 'San Antonio': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def brac_keywords(text):\n",
    "    acronym = 'brac' in text.lower()\n",
    "    full = 'base realignment and closure' in text.lower()\n",
    "    return acronym or full\n",
    "brac_articles = {city: brac_keywords(text) for city, text in articles.items()}\n",
    "print('Is BRAC discussed? (should be all True):')\n",
    "brac_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e28d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is economic uncertainty discussed?:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Atlanta': False,\n",
       " 'Baltimore 1': False,\n",
       " 'Baltimore 2': False,\n",
       " 'Indianapolis': False,\n",
       " 'Lubbock': False,\n",
       " 'Norfolk': True,\n",
       " 'Norwich 1': False,\n",
       " 'Norwich 2': False,\n",
       " 'Richmond': False,\n",
       " 'San Antonio': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unc_keywords(text):\n",
    "    unc = 'uncertain' in text.lower()\n",
    "    econ = 'econom' in text.lower()\n",
    "    return unc and econ\n",
    "brac_articles = {city: unc_keywords(text) for city, text in articles.items()}\n",
    "print('Is economic uncertainty discussed?:')\n",
    "brac_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac9243",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "First we insert the sentiment scoring into the pipline, then injest the data into Spacy to tokenize the contents. Sentiment scores range from 1 (positive) to -1 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6c60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob');\n",
    "\n",
    "docs = {city:nlp(text) for city, text in articles.items()} #turning plain text into Spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6222c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment (higher = more positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Lubbock', 0.0835),\n",
       " ('Baltimore 2', 0.0673),\n",
       " ('San Antonio', 0.0527),\n",
       " ('Norfolk', 0.0394),\n",
       " ('Richmond', 0.0348),\n",
       " ('Indianapolis', -0.0015),\n",
       " ('Baltimore 1', -0.0108),\n",
       " ('Norwich 1', -0.0903),\n",
       " ('Atlanta', -0.1481),\n",
       " ('Norwich 2', -0.1697)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_polarity = {city:round(doc._.blob.polarity, 4)  for city, doc in docs.items()}\n",
    "print('Overall sentiment (higher = more positive)')\n",
    "sorted(doc_polarity.items(), key=lambda docs: docs[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a0c2f",
   "metadata": {},
   "source": [
    "To compare the sentiment score to the actual proposed job losses, I will take the averages and merge them all together. They align fairly well. Atlanta and Norwich both have massive negative results, and are at the bottom of the sentiment rankings. This is fairly rudimentary, but might be enough text analysis for this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94819067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the two articles for Baltimore and Norwich into an average for each\n",
    "doc_polarity['Baltimore'] = round(float(np.mean([doc_polarity['Baltimore 1'], doc_polarity['Baltimore 2']])), 4)\n",
    "doc_polarity['Norwich'] = round(float(np.mean([doc_polarity['Norwich 1'], doc_polarity['Norwich 2']])), 4)\n",
    "del doc_polarity['Baltimore 1'], doc_polarity['Baltimore 2'], doc_polarity['Norwich 1'], doc_polarity['Norwich 2']\n",
    "\n",
    "#convert to a dataframe in order to merge and display neatly\n",
    "polarity = pd.DataFrame({'city':doc_polarity.keys(), 'doc_sentiment':doc_polarity.values()})\n",
    "df_brac = df_brac.merge(polarity, on='city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9e11b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>jobs_lost</th>\n",
       "      <th>jobs_gained</th>\n",
       "      <th>net</th>\n",
       "      <th>doc_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lubbock</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>-7</td>\n",
       "      <td>0.0835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>San Antonio</td>\n",
       "      <td>-8129</td>\n",
       "      <td>10946</td>\n",
       "      <td>2817</td>\n",
       "      <td>0.0527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norfolk</td>\n",
       "      <td>-10399</td>\n",
       "      <td>10400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>-812</td>\n",
       "      <td>7821</td>\n",
       "      <td>7009</td>\n",
       "      <td>0.0348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baltimore</td>\n",
       "      <td>-4414</td>\n",
       "      <td>11619</td>\n",
       "      <td>7727</td>\n",
       "      <td>0.0282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>-283</td>\n",
       "      <td>3592</td>\n",
       "      <td>3309</td>\n",
       "      <td>-0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norwich</td>\n",
       "      <td>-8460</td>\n",
       "      <td>0</td>\n",
       "      <td>-8460</td>\n",
       "      <td>-0.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>-6820</td>\n",
       "      <td>124</td>\n",
       "      <td>-6764</td>\n",
       "      <td>-0.1481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city  jobs_lost  jobs_gained   net  doc_sentiment\n",
       "2       Lubbock         -7            0    -7         0.0835\n",
       "6   San Antonio      -8129        10946  2817         0.0527\n",
       "3       Norfolk     -10399        10400     1         0.0394\n",
       "5      Richmond       -812         7821  7009         0.0348\n",
       "1     Baltimore      -4414        11619  7727         0.0282\n",
       "7  Indianapolis       -283         3592  3309        -0.0015\n",
       "4       Norwich      -8460            0 -8460        -0.1300\n",
       "0       Atlanta      -6820          124 -6764        -0.1481"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brac.sort_values('doc_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413566ac",
   "metadata": {},
   "source": [
    "# Exploring sentiment about specific topics\n",
    "\n",
    "Text analysis can do named entity recognition based on pre-trained libraries. ORG=organization, GEP=global-political entities, LOC=non-GPE locations. This is an area where we may want to train our own model, at least in part, since we can do things like link different entities together (e.g. base realignment and closure linked to BRAC, or all military base names linked together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ab9bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Pentagon, 'ORG'),\n",
       " (the Naval Surface Warfare Center at Crane, 'ORG'),\n",
       " (Defense Finance and Accounting Service, 'ORG'),\n",
       " (Base Realignment and Closure, 'ORG'),\n",
       " (Crane, 'ORG'),\n",
       " (Defense Finance and Accounting Service, 'ORG'),\n",
       " (DFAS, 'ORG'),\n",
       " (the Department of Defense, 'ORG'),\n",
       " (Soldier Support Center, 'ORG'),\n",
       " (Army, 'ORG'),\n",
       " (Crane, 'ORG'),\n",
       " (Daniels, 'ORG'),\n",
       " (Crane, 'ORG'),\n",
       " (Crane, 'ORG'),\n",
       " (Crane, 'ORG'),\n",
       " (Pentagon, 'ORG'),\n",
       " (Crane, 'ORG'),\n",
       " (Sodrel, 'ORG'),\n",
       " (CB Richard Ellis, 'ORG'),\n",
       " (the West Gate Technology Park at, 'ORG'),\n",
       " (Busch, 'ORG'),\n",
       " (the Hulman Regional Airport Air Guard Station, 'ORG'),\n",
       " (Navy Marine Corps, 'ORG'),\n",
       " (the Newport Chemical Depot, 'ORG'),\n",
       " (the Navy Reserve Center, 'ORG'),\n",
       " (the U.S. Army Reserve Center, 'ORG'),\n",
       " (the U.S. Army Reserve Center, 'ORG'),\n",
       " (Congress, 'ORG'),\n",
       " (Daniels, 'ORG')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Focusing on just ORGs identified in the article for Indianapolis\n",
    "orgs = [(entity, entity.label_) for entity in docs['Indianapolis'].ents if entity.label_ == 'ORG']\n",
    "orgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5191f79",
   "metadata": {},
   "source": [
    "Nearly all of these appear to be military related, but we would need to do more refinement to make sure. That is not technically difficult to do at all, but would take some time.\n",
    "\n",
    "We can get the average sentiment relating to only organizations using filtering on entities, though the results are rough without more work on entitity identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "242c0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment around all organizations in the articles\n",
    "docs_org_sentiment = {city:float(round(np.mean([\n",
    "    entity._.blob.polarity for entity in doc.ents if entity.label_ == 'ORG']), 4))\n",
    "      for city, doc in docs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec4f069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the two articles for Baltimore and Norwich into an average for each\n",
    "docs_org_sentiment['Baltimore'] = round(float(np.mean([docs_org_sentiment['Baltimore 1'], docs_org_sentiment['Baltimore 2']])), 4)\n",
    "docs_org_sentiment['Norwich'] = round(float(np.mean([docs_org_sentiment['Norwich 1'], docs_org_sentiment['Norwich 2']])), 4)\n",
    "del docs_org_sentiment['Baltimore 1'], docs_org_sentiment['Baltimore 2'], docs_org_sentiment['Norwich 1'], docs_org_sentiment['Norwich 2']\n",
    "\n",
    "#convert to a dataframe in order to merge and display neatly\n",
    "polarity = pd.DataFrame({'city':docs_org_sentiment.keys(), 'org_sentiment':docs_org_sentiment.values()})\n",
    "df_brac = df_brac.merge(polarity, on='city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f02d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>jobs_lost</th>\n",
       "      <th>jobs_gained</th>\n",
       "      <th>net</th>\n",
       "      <th>doc_sentiment</th>\n",
       "      <th>org_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norfolk</td>\n",
       "      <td>-10399</td>\n",
       "      <td>10400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baltimore</td>\n",
       "      <td>-4414</td>\n",
       "      <td>11619</td>\n",
       "      <td>7727</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>-0.0140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>-283</td>\n",
       "      <td>3592</td>\n",
       "      <td>3309</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>-812</td>\n",
       "      <td>7821</td>\n",
       "      <td>7009</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>-0.0598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norwich</td>\n",
       "      <td>-8460</td>\n",
       "      <td>0</td>\n",
       "      <td>-8460</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>-0.0645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lubbock</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>-7</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>-0.0654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>-6820</td>\n",
       "      <td>124</td>\n",
       "      <td>-6764</td>\n",
       "      <td>-0.1481</td>\n",
       "      <td>-0.0667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>San Antonio</td>\n",
       "      <td>-8129</td>\n",
       "      <td>10946</td>\n",
       "      <td>2817</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>-0.0731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city  jobs_lost  jobs_gained   net  doc_sentiment  org_sentiment\n",
       "3       Norfolk     -10399        10400     1         0.0394         0.0000\n",
       "1     Baltimore      -4414        11619  7727         0.0282        -0.0140\n",
       "7  Indianapolis       -283         3592  3309        -0.0015        -0.0448\n",
       "5      Richmond       -812         7821  7009         0.0348        -0.0598\n",
       "4       Norwich      -8460            0 -8460        -0.1300        -0.0645\n",
       "2       Lubbock         -7            0    -7         0.0835        -0.0654\n",
       "0       Atlanta      -6820          124 -6764        -0.1481        -0.0667\n",
       "6   San Antonio      -8129        10946  2817         0.0527        -0.0731"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brac.sort_values('org_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cdbbc",
   "metadata": {},
   "source": [
    "I think sentiment specifically about military bases is going to be our best bet, but one advantage to also looking at GPEs is that some articles are about places other than where they are published. For example, the Lubbock, TX article is ranked poorly now, but looking at the text makes it clear it is about the BRAC process affecting a town in Nevada, even though it was published in Lubbock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6eea941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HAWTHORNE,\n",
       " Nev.,\n",
       " America,\n",
       " Nevada,\n",
       " Reno,\n",
       " Nevada,\n",
       " Iraq,\n",
       " Hawthorne,\n",
       " U.S.,\n",
       " Utah,\n",
       " Hawthorne,\n",
       " El Capitan,\n",
       " Hawthorne]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_gpes = {city:[entity for entity in doc.ents if entity.label_ == 'GPE'] for city, doc in docs.items()}\n",
    "docs_gpes['Lubbock']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd809ed5",
   "metadata": {},
   "source": [
    "Here we see the GPEs in the Lubbock article. If we eliminate the country-level names American, USA, and Iraq, we're left with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b90760de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hawthorne',\n",
       " 'nev.',\n",
       " 'nevada',\n",
       " 'reno',\n",
       " 'nevada',\n",
       " 'hawthorne',\n",
       " 'utah',\n",
       " 'hawthorne',\n",
       " 'el capitan',\n",
       " 'hawthorne']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lubbock_gpes = [gpe.text.lower() for gpe in docs_gpes['Lubbock'] if gpe.text not in ['U.S.', 'America', 'Iraq']]\n",
    "lubbock_gpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776b3c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hawthorne': 4,\n",
       "         'nevada': 2,\n",
       "         'nev.': 1,\n",
       "         'reno': 1,\n",
       "         'utah': 1,\n",
       "         'el capitan': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(lubbock_gpes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32afda95",
   "metadata": {},
   "source": [
    "The article is about Hawthorne, Nevada, meaning 7 of the 10 GPEs mentioned direct us to the proper location. Of the remaining GPEs, \"reno\" is used to describe the location of Hawthorn in relation to a major city, \"utah\" is mentioned as the destination for the jobs Hawthorn is losing, and \"El Capitan\" is the name of a casino (the second largest employer in Hawthorn) that the entity parser is incorrectly identifying as the location of the mountain by the same name.  \n",
    "\n",
    "When we look at the top three results for all of the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d24aede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atlanta': [('clayton county', 2), ('georgia', 2), ('atlanta', 1)],\n",
       " 'Baltimore 1': [('washington', 6), ('maryland', 5), ('baltimore', 4)],\n",
       " 'Baltimore 2': [('maryland', 4), ('washington', 3), ('va.', 3)],\n",
       " 'Indianapolis': [('indiana', 8), ('lawrence', 2), ('indianapolis', 2)],\n",
       " 'Lubbock': [('hawthorne', 4), ('nevada', 2), ('nev.', 1)],\n",
       " 'Norfolk': [('virginia', 4), ('fort', 2), ('hampton', 2)],\n",
       " 'Norwich 1': [('norfolk', 3), ('ga.', 1), ('virginia', 1)],\n",
       " 'Norwich 2': [('connecticut', 4), ('washington', 1), ('middletown', 1)],\n",
       " 'Richmond': [('petersburg', 2), ('richmond', 1), ('fort', 1)],\n",
       " 'San Antonio': [(\"san antonio's\", 2), ('pennsylvania', 1), ('lackland', 1)]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{city:Counter([\n",
    "    gpe.text.lower() for gpe in doc if gpe.text not in ['U.S.', 'America', 'Iraq']\n",
    "        ]).most_common(3) for city, doc in docs_gpes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72d6c3",
   "metadata": {},
   "source": [
    "It seems that a simple heuristic, like finding the most referred-to geographies, will generally discover the correct geographic location the article is focused on. We would have to refine our geographic linking so that we identify, for example, that \"clayton county\", \"georiga\", and \"atlanta\" are all referring to the same place for the purposes of the first article.\n",
    "\n",
    "Looking at GPEs in articles also opens the door to studying spillover effects and economic linkages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3bae9",
   "metadata": {},
   "source": [
    "# Conclusions and Issues\n",
    "\n",
    "There is always a tradeoff in text analysis between accuracy and work time/computing power. Effort spent parsing the meaning out of text has major diminishing returns, but I think it is accurate to say that it only asymptotically approaches zero for any article of sufficient size. In my experience there is always going to be one more corner case that we could correct for, but at some point it is better to leave them as noise. For example, it may be rare for articles on BRAC to refer only to other places (like the Lubbock, TX article on Hancock, NV), so it may not be worth our time to put a lot of effort into correcting a very small number of articles. \n",
    "\n",
    "A second related issue that frequently arises in the text processing space is that there are many tools and many judgment calls involved, often with very limited ability to defend to a referee that one is the right approach over another. For example, here I used a pre-trained sentiment algorithm from textblob, which is a popular choice. But we could also use another library, like huggingface, which would give us access to a whole new subset of sentiment algorithms. We could also put in the effort to train our own custom algorithm, though in this case I do not think that would be worth our time, even if we had the resources for it. \n",
    "\n",
    "The many possible approaches makes our results harder to defend, and we may have to offer multiple versions of the results using different algorithms that hopefully align with their conclusions. Ideally we would assess our machine-driven results by comparing them to manual classification done by RAs, but that would be a substantial task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27041b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
